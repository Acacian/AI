import os
import json
import yaml
import requests
from kafka import KafkaConsumer, KafkaProducer
from gateway.triton.triton_client import TritonClient

# ì„¤ì • ë¡œë”©
with open("agents_llm/config.yaml", "r") as f:
    config = yaml.safe_load(f)

# Triton client ì´ˆê¸°í™”
triton = TritonClient()

# Kafka ì„¤ì •
consumer = KafkaConsumer(
    config["listen_topic"],
    bootstrap_servers=os.getenv("KAFKA_BROKER", "kafka:9092"),
    group_id=config.get("group_id", "llm_agent_group"),
    value_deserializer=lambda m: json.loads(m.decode("utf-8")),
    auto_offset_reset="latest",
)

producer = KafkaProducer(
    bootstrap_servers=os.getenv("KAFKA_BROKER", "kafka:9092"),
    value_serializer=lambda v: json.dumps(v).encode("utf-8"),
)

# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(signal_data: dict) -> str:
    try:
        return config["prompt_template"].format(**signal_data)
    except KeyError as e:
        missing = str(e).strip("'")
        raise ValueError(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: ëˆ„ë½ëœ í•„ë“œ '{missing}'")

# LLM í˜¸ì¶œ í•¨ìˆ˜
def query_llm(prompt: str) -> str:
    res = requests.post(config["llm_api_url"], json={
        "model": config["model_name"],
        "prompt": prompt,
        "max_tokens": config["max_tokens"],
        "temperature": config["temperature"],
        "stop": ["\n"]
    }, timeout=10)

    if res.status_code != 200:
        raise RuntimeError(f"LLM ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}, {res.text}")

    result = res.json()
    return result.get("choices", [{}])[0].get("text", "").strip()

# ë©”ì¸ ë£¨í”„
def run():
    print(f"ğŸ§  LLM Agent ì‹œì‘: topic={config['listen_topic']}", flush=True)
    for msg in consumer:
        try:
            data = msg.value
            if "input" not in data:
                print(f"âš ï¸ 'input' í•„ë“œ ëˆ„ë½ë¨: {data}", flush=True)
                continue

            # signal feature ê¸°ë°˜ ì¶”ë¡ 
            models = ["pattern_ae", "risk_scorer", "volume_ae", "trend_segmenter", "volatility_watcher"]
            signal = triton.infer_signal(data, models)
            prompt = build_prompt(signal)
            decision = query_llm(prompt)
            print(f"âœ… ì „ëµ ê²°ì •: {decision}", flush=True)

            producer.send(config["output_topic"], {
                "input": data,
                "signal": signal,
                "decision": decision,
                "raw_prompt": prompt
            })
            producer.flush()

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", flush=True)

if __name__ == "__main__":
    run()

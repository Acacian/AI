import json
import yaml
import time
import requests
import numpy as np
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import NoBrokersAvailable
import tritonclient.http as httpclient

# TritonClient í´ë˜ìŠ¤
class TritonClient:
    def __init__(self, url: str = "triton:8000"):
        self.client = httpclient.InferenceServerClient(url=url)

    def preprocess(self, data: dict) -> np.ndarray:
        features = data.get("input")
        if not isinstance(features, list):
            raise ValueError("âŒ Triton ì…ë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        x = np.array(features, dtype=np.float32)
        if x.ndim == 2:
            x = np.expand_dims(x, axis=0)  # [1, seq_len, dim]
        return x

    def postprocess(self, output: np.ndarray) -> list:
        return output.tolist()

    def infer(self, model_name: str, data: dict) -> list:
        x = self.preprocess(data)
        inputs = [httpclient.InferInput("INPUT", x.shape, "FP32")]
        inputs[0].set_data_from_numpy(x)
        outputs = [httpclient.InferRequestedOutput("OUTPUT")]
        result = self.client.infer(model_name, inputs=inputs, outputs=outputs)
        return self.postprocess(result.as_numpy("OUTPUT"))

    def infer_signal(self, data: dict, models: list[str]) -> dict:
        signal = {}
        for model_name in models:
            try:
                output = self.infer(model_name, data)
                if isinstance(output, list) and len(output) == 1 and isinstance(output[0], list):
                    signal[model_name] = output[0]
                else:
                    signal[model_name] = output
            except Exception as e:
                print(f"âŒ ëª¨ë¸ '{model_name}' ì¶”ë¡  ì‹¤íŒ¨: {e}")
                signal[model_name] = None
        return signal

# ì„¤ì • ë¡œë”©
with open("agents_llm/config.yaml", "r") as f:
    config = yaml.safe_load(f)

# KafkaConsumer ì—°ê²°
def init_consumer_with_retry(config: dict, max_retries: int = 10, delay_sec: int = 5):
    for attempt in range(max_retries):
        try:
            consumer = KafkaConsumer(
                config["listen_topic"],
                bootstrap_servers=config.get("kafka_broker", "kafka:9092"),
                group_id=config.get("group_id", "llm_agent_group"),
                value_deserializer=lambda m: json.loads(m.decode("utf-8")),
                auto_offset_reset="latest",
            )
            print("âœ… Kafka ì—°ê²° ì„±ê³µ")
            return consumer
        except NoBrokersAvailable:
            print(f"ğŸ” Kafka ì—°ê²° ì¬ì‹œë„ ì¤‘... ({attempt+1}/{max_retries})")
            time.sleep(delay_sec)
    raise RuntimeError("âŒ Kafka ì—°ê²° ì‹¤íŒ¨: No brokers available")

consumer = init_consumer_with_retry(config)

# KafkaProducer ìƒì„±
producer = KafkaProducer(
    bootstrap_servers=config.get("kafka_broker", "kafka:9092"),
    value_serializer=lambda v: json.dumps(v).encode("utf-8"),
)

# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(signal_data: dict) -> str:
    try:
        return config["prompt_template"].format(**signal_data)
    except KeyError as e:
        missing = str(e).strip("'")
        raise ValueError(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: ëˆ„ë½ëœ í•„ë“œ '{missing}'")

# LLM í˜¸ì¶œ í•¨ìˆ˜
def query_llm(prompt: str) -> str:
    res = requests.post(config["llm_api_url"], json={
        "model": config["model_name"],
        "prompt": prompt,
        "max_tokens": config["max_tokens"],
        "temperature": config["temperature"],
        "stop": ["\n"]
    }, timeout=10)

    if res.status_code != 200:
        raise RuntimeError(f"LLM ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}, {res.text}")

    result = res.json()
    return result.get("choices", [{}])[0].get("text", "").strip()

# ë©”ì¸ ë£¨í”„
def run():
    print(f"ğŸ§  LLM Agent ì‹œì‘: topic={config['listen_topic']}", flush=True)
    triton = TritonClient()

    for msg in consumer:
        try:
            data = msg.value
            if "input" not in data:
                print(f"âš ï¸ 'input' í•„ë“œ ëˆ„ë½ë¨: {data}", flush=True)
                continue

            models = ["pattern_ae", "risk_scorer", "volume_ae", "trend_segmenter", "volatility_watcher"]
            signal = triton.infer_signal(data, models)
            prompt = build_prompt({
                "pattern": signal.get("pattern_ae"),
                "risk": signal.get("risk_scorer"),
                "volume": signal.get("volume_ae"),
                "trend": signal.get("trend_segmenter"),
                "volatility": signal.get("volatility_watcher"),
            })
            decision = query_llm(prompt)
            print(f"âœ… ì „ëµ ê²°ì •: {decision}", flush=True)

            producer.send(config["output_topic"], {
                "input": data,
                "signal": signal,
                "decision": decision,
                "raw_prompt": prompt
            })
            producer.flush()

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", flush=True)

if __name__ == "__main__":
    run()
